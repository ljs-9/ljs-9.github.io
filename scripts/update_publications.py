import json
import requests
from datetime import datetime
import os
import time
import re

# Google Scholar ID
SCHOLAR_ID = "UdIP7WoAAAAJ"

# ä» GitHub Secret è¯»å– SerpAPI key
API_KEY = os.getenv("SERPAPI_KEY")
if not API_KEY:
    raise ValueError("âŒ Missing SERPAPI_KEY. Please add it as a GitHub Secret.")

URL = f"https://serpapi.com/search.json?engine=google_scholar_author&author_id={SCHOLAR_ID}&api_key={API_KEY}"

print("ğŸ” Fetching publications from SerpAPI...")
r = requests.get(URL)
if r.status_code != 200:
    raise Exception(f"âŒ API request failed: {r.status_code} - {r.text}")

data = r.json()
articles = data.get("articles", [])

# è¾“å‡ºè·¯å¾„
output_path = "data/publications.json"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# åŠ è½½æ—§æ•°æ®ï¼ˆç¼“å­˜ï¼‰
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        old_data = {pub["title"]: pub for pub in json.load(f)}
else:
    old_data = {}

def clean_text(s: str) -> str:
    """æ¸…ç†å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šç¬¦å·"""
    return re.sub(r"[^A-Za-z0-9\s\-&]", "", s).strip()

def fetch_doi_from_crossref(title, authors="", year=""):
    """é€šè¿‡ CrossRef ç²¾å‡†åŒ¹é… DOI"""
    title_clean = clean_text(title)
    author_first = authors.split(",")[0] if authors else ""

    # ç¬¬ä¸€æ¬¡ç²¾ç¡®åŒ¹é…ï¼štitle + author + year
    query = f"{title_clean} {author_first} {year}".strip()
    url = f"https://api.crossref.org/works?query={requests.utils.quote(query)}&rows=1"

    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            print(f"âš ï¸ CrossRef request failed ({res.status_code}) for: {title}")
            return ""

        items = res.json().get("message", {}).get("items", [])
        if items:
            item = items[0]
            doi = item.get("DOI", "")
            found_title = item.get("title", [""])[0]
            print(f"    âœ… Found DOI: {doi}")
            print(f"       â†³ Matched title: {found_title}")
            return doi

        # ç¬¬äºŒæ¬¡å°è¯•ï¼šä»…ç”¨æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…
        fallback_url = f"https://api.crossref.org/works?query.title={requests.utils.quote(title_clean)}&rows=1"
        res2 = requests.get(fallback_url, timeout=10)
        items2 = res2.json().get("message", {}).get("items", [])
        if items2:
            item = items2[0]
            doi = item.get("DOI", "")
            found_title = item.get("title", [""])[0]
            print(f"    âœ… Found DOI (fallback): {doi}")
            print(f"       â†³ Matched title: {found_title}")
            return doi

        print("    âŒ No DOI match found.")
    except Exception as e:
        print(f"âš ï¸ Error while fetching DOI for '{title}': {e}")
    return ""

publications = []
for i, pub in enumerate(articles, start=1):
    title = pub.get("title", "")
    authors = pub.get("authors", "")
    year = str(pub.get("year", ""))
    journal = pub.get("publication", "")
    citations = pub.get("cited_by", {}).get("value", 0)
    pdf = pub.get("link", "")

    # ä½¿ç”¨ç¼“å­˜ä¸­çš„ DOI
    doi = ""
    if title in old_data and old_data[title].get("doi"):
        doi = old_data[title]["doi"]
        print(f"ğŸŸ¢ [{i}/{len(articles)}] Cached DOI found for: {title}")
    else:
        print(f"ğŸ”¹ [{i}/{len(articles)}] Fetching DOI for: {title}")
        doi = fetch_doi_from_crossref(title, authors, year)
        time.sleep(1.5)  # é˜²æ­¢ CrossRef é™æµ

    publications.append({
        "title": title,
        "authors": authors,
        "year": year,
        "journal": journal,
        "pages": "",
        "citations": citations,
        "doi": doi,
        "pdf": pdf
    })

# ä¿å­˜ JSON æ–‡ä»¶
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(publications, f, ensure_ascii=False, indent=2)

print(f"\nâœ… Updated {len(publications)} publications (DOI included where available).")
print(f"ğŸ“… Last updated: {datetime.now()}")
